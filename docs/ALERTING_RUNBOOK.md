# DocN Alerting System Runbook

## üìã Overview

This runbook provides procedures for responding to alerts generated by the DocN monitoring and alerting system.

**Version**: 1.0  
**Last Updated**: January 2026

---

## üö® Alert Severity Levels

### Critical (Red)
- **Response Time**: Immediate (< 15 minutes)
- **Impact**: System degradation or failure
- **Escalation**: Notify on-call engineer immediately

### Warning (Yellow)
- **Response Time**: Within 1 hour
- **Impact**: Potential issues or degradation
- **Escalation**: Notify team during business hours

### Info (Blue)
- **Response Time**: Next business day
- **Impact**: Informational only
- **Escalation**: None required

---

## üìä Common Alerts

### 1. HighCPU

**Severity**: Critical  
**Description**: CPU usage above 90% for 5+ minutes

**Diagnostic Steps**:
1. Check system metrics: `GET /api/metrics/alerts`
2. Identify top CPU consumers in Hangfire dashboard: `/hangfire`
3. Check for runaway background jobs
4. Review recent deployments

**Resolution**:
- **Short-term**: 
  - Kill non-critical background jobs
  - Restart application if necessary
  - Scale horizontally if in cloud environment
- **Long-term**:
  - Optimize identified CPU-intensive operations
  - Add more worker instances
  - Review and optimize algorithms

**Related Logs**: Check `/logs` endpoint for errors

---

### 2. HighMemory

**Severity**: Critical  
**Description**: Memory usage above 90% for 5+ minutes

**Diagnostic Steps**:
1. Check memory metrics: `GET /api/metrics/alerts`
2. Review Hangfire jobs for memory leaks
3. Check document processing queue size
4. Review embedding generation queue

**Resolution**:
- **Short-term**:
  - Restart application to free memory
  - Clear Redis cache if configured
  - Reduce concurrent job processing
- **Long-term**:
  - Implement better memory management for large documents
  - Add memory limits to background jobs
  - Scale vertically (add more RAM)

**Prevention**: Configure maximum document size limits

---

### 3. HighLatency

**Severity**: Warning  
**Description**: P95 API latency above 2 seconds for 5+ minutes

**Diagnostic Steps**:
1. Check latency breakdown: `GET /api/metrics/alerts`
2. Identify slow endpoints from metrics
3. Check database connection pool usage
4. Review AI provider response times

**Resolution**:
- **Immediate**:
  - Check AI provider status
  - Verify database connectivity
  - Review current load
- **Short-term**:
  - Enable request caching for common queries
  - Optimize slow database queries
- **Long-term**:
  - Implement CDN for static content
  - Add query result caching
  - Optimize vector search queries

**Monitoring**: Use OpenTelemetry traces to identify bottlenecks

---

### 4. HighErrorRate

**Severity**: Critical  
**Description**: Error rate above 5% for 5+ minutes

**Diagnostic Steps**:
1. Check error metrics: `GET /api/metrics/alerts`
2. Review recent errors: `GET /api/logs?level=Error&count=50`
3. Check health endpoints: `GET /health`
4. Verify external service connectivity (AI providers, database)

**Resolution**:
- **Immediate**:
  - Check if specific endpoint is failing
  - Verify database connectivity
  - Check AI provider API keys and quotas
- **Short-term**:
  - Roll back recent deployment if error spike started after deploy
  - Add rate limiting to failing endpoints
  - Enable circuit breaker for external services
- **Long-term**:
  - Improve error handling and retries
  - Add input validation
  - Implement better fallback mechanisms

**Critical**: If error rate > 25%, consider taking system offline for maintenance

---

### 5. UnhandledException

**Severity**: Critical  
**Description**: Unhandled exception in application

**Diagnostic Steps**:
1. Check exception details in alert
2. Review stack trace in logs
3. Identify affected endpoint
4. Check for data corruption

**Resolution**:
- **Immediate**:
  - Fix critical bug if obvious
  - Add try-catch block to prevent cascade failures
- **Short-term**:
  - Deploy hotfix
  - Add monitoring for similar errors
- **Long-term**:
  - Improve error handling
  - Add comprehensive unit tests
  - Implement better input validation

---

### 6. LowRAGQuality

**Severity**: Warning  
**Description**: RAG response quality below threshold

**Diagnostic Steps**:
1. Check quality metrics: `GET /api/rag-quality/metrics`
2. Review recent low-quality responses
3. Check RAGAS scores: `GET /api/rag-quality/ragas/monitoring`
4. Verify document quality in database

**Resolution**:
- **Immediate**:
  - Check if specific document causing issues
  - Verify embeddings are generated correctly
- **Short-term**:
  - Re-process problematic documents
  - Adjust RAG configuration parameters
  - Update prompt templates
- **Long-term**:
  - Improve document preprocessing
  - Fine-tune embedding model
  - Enhance retrieval algorithms

---

## üîß Diagnostic Tools

### Health Check Endpoints
```bash
# Overall health
curl https://localhost:5211/health

# Liveness probe
curl https://localhost:5211/health/live

# Readiness probe
curl https://localhost:5211/health/ready
```

### Metrics Endpoints
```bash
# Prometheus metrics
curl https://localhost:5211/metrics

# Alert metrics
curl https://localhost:5211/api/metrics/alerts

# RAG quality dashboard
curl https://localhost:5211/api/rag-quality/dashboard
```

### Alert Management
```bash
# Get active alerts
curl https://localhost:5211/api/alerts/active

# Acknowledge alert
curl -X POST https://localhost:5211/api/alerts/{alertId}/acknowledge \
  -H "Content-Type: application/json" \
  -d '{"acknowledgedBy": "your-name"}'

# Resolve alert
curl -X POST https://localhost:5211/api/alerts/{alertId}/resolve \
  -H "Content-Type: application/json" \
  -d '{"resolvedBy": "your-name"}'
```

---

## üìû Escalation Procedures

### Level 1: On-Call Engineer
- **Response Time**: 15 minutes
- **Handles**: All Critical alerts
- **Escalates to**: Level 2 if unresolved in 1 hour

### Level 2: Senior Engineer / Team Lead
- **Response Time**: 30 minutes
- **Handles**: Complex issues, architectural problems
- **Escalates to**: Level 3 if system-wide outage

### Level 3: Engineering Manager
- **Response Time**: 1 hour
- **Handles**: Major outages, vendor escalations
- **Authority**: Can approve emergency changes

---

## üìù Post-Incident Procedures

After resolving a critical alert:

1. **Document the incident**:
   - What happened
   - Root cause
   - Resolution steps taken
   - Time to resolution

2. **Create action items**:
   - Prevent recurrence
   - Improve monitoring
   - Update runbook

3. **Conduct post-mortem** (if major incident):
   - Timeline of events
   - Impact assessment
   - Lessons learned
   - Action items with owners

4. **Update monitoring**:
   - Add alerts for similar issues
   - Adjust thresholds if needed
   - Improve dashboards

---

## üîó Quick Links

- **Monitoring Dashboard**: `/hangfire`
- **API Documentation**: `/swagger`
- **Logs**: `GET /api/logs`
- **Health Status**: `/health`
- **Alert Status**: `GET /api/alerts/active`

---

## üìö Additional Resources

- [DocN Architecture Documentation](../README.md)
- [Deployment Guide](../DEPLOYMENT_GUIDE.md)
- [Monitoring & Observability](../MONITORING_OBSERVABILITY.md)
- [GAP Analysis](../GAP_ANALYSIS_E_RACCOMANDAZIONI.md)

---

**Remember**: When in doubt, it's better to escalate early than to let an issue persist!
